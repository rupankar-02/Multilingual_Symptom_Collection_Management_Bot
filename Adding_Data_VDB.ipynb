{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import hashlib\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import ast\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Step 1: Connect to the existing Pinecone index\n",
        "def connect_to_pinecone(api_key, index_name=\"patient-symptoms\"):\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    return pc.Index(index_name)\n",
        "\n",
        "# Pinecone API key and index configuration\n",
        "api_key = \"00dfadae-35e0-4fcd-92b7-f88e21899500\"  # Replace with your Pinecone API key\n",
        "index_name = \"patient-symptoms\"\n",
        "\n",
        "# Connect to Pinecone index\n",
        "index = connect_to_pinecone(api_key, index_name)\n",
        "\n",
        "# Load embedding model\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "embedding_model = AutoModel.from_pretrained(embedding_model_name).to('cuda')\n",
        "\n",
        "# Step 2: Function to generate embeddings\n",
        "def generate_embeddings(text):\n",
        "    inputs = embedding_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
        "    embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings.detach().cpu().numpy()\n",
        "\n",
        "# Step 3: Function to upsert data\n",
        "def store_in_vector_database(index, patient_name, symptoms):\n",
        "    for symptom in symptoms:\n",
        "        # Generate embedding vector for each symptom\n",
        "        embedding_vector = generate_embeddings(symptom)\n",
        "\n",
        "        # Generate a unique ID for the patient-symptom pair using MD5 hashing\n",
        "        unique_id = hashlib.md5(f\"{patient_name}-{symptom}\".encode()).hexdigest()\n",
        "\n",
        "        # Upsert the data into Pinecone with patient name and symptom as metadata\n",
        "        index.upsert([(unique_id, embedding_vector.flatten(), {\"patient_name\": patient_name, \"symptom\": symptom})])\n",
        "\n",
        "# Step 4: Load and process the dataset\n",
        "file_path = \"synthetic_patient_data_with_random_names.csv\"  # Path to your dataset\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "for _, row in data.iterrows():\n",
        "    patient_name = row['name']\n",
        "    # Parse symptoms column from string representation of a list\n",
        "    symptoms = ast.literal_eval(row['extracted_symptoms'])\n",
        "    # Remove numbering from symptoms for a clean input to Pinecone\n",
        "    cleaned_symptoms = [symptom.split(\". \", 1)[-1] for symptom in symptoms]\n",
        "    # Upsert data into Pinecone\n",
        "    store_in_vector_database(index, patient_name, cleaned_symptoms)\n",
        "\n",
        "print(\"Data successfully upserted into Pinecone.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJva-7uCYVbl",
        "outputId": "b358d98d-eaa1-4077-eff8-3b05d5dd7dbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully upserted into Pinecone.\n"
          ]
        }
      ]
    }
  ]
}